
# ML 簡易入門

ML (Machine Learning) 是一門高深複雜的學問。

這篇的目的主要是希望沒有基礎的人也能在需要應用相關技術時不至於像無頭蒼蠅一樣，只要稍有概念即可。

## ML的概念

簡單來說，可以把 ML 想成尋找一個特定的"函數" (也就是 **模型** **model**)，而這個函數可能是吃聲音資訊，輸出一個人名 (語音辨識) 、或是吃一張圖片，輸出一個物品名稱 (影像辨識)。

一般而言，日常的 ML 應用比較常應用到的模型類別 (可以想成函數) 有兩種，一種是輸出一個數值，也就是 **回歸模型**，例如: 一個預測明天 PM2.5 數值的模型，其有今日 PM2.5 數值、今日溫度之類的數值當作輸入，最後輸出預測明天 PM2.5 的數值，衡量這種回歸模型的好壞通常是用 **loss** (想成誤差，雖然 loss 有很多不同算法) 來評斷，loss 要越小越好。

另一種是輸出一個類別，也就是 **分類模型**，例如: 垃圾電子郵件過濾系統是把電子郵件作為輸入，然後輸出是否為垃圾郵件的 yes or no ，這種屬於 binary classification，或是輸入一種動物圖片，模型輸出一種動物名稱，這種就是 multi-class 的分類，而分類的好壞主要是看分類的 **正確率**。當然除了這兩大類模型之外還有很多其他的模型，像是自動語音翻譯要用的模型之類的，不過一般應用這兩種比較多。

## ML 的流程

要找到一個能解決問題的模型大抵的流程是這樣。

 1. 取得資料 (data)  
 2. 選擇模型、選擇參數
 3. 開始訓練
 4. 訓練結束測試模型
 5. 應用模型
 
 ### 取得資料 (data)  
 
資料在不要有太多錯誤資料 (像是可能感測器壞了所收到的數據跟實際不一樣或是有很多答案類別標錯的資料) 的前提下，資料當然是越多越好。一般來說要訓練前通常會把資料分成專門丟入模型訓練的"**訓練資料**" (**training data**) 跟 訓練模型完畢後測試模型用的 "**測試資料**" (**validation set**) 以避免 **overfit** (在訓練資料上跑時結果很好但是實際應用很差)，至於要怎麼分可以自己視資料大小決定。

另外，資料在倒入模型訓練之前，也可以視情況選擇對資料進行前處理 (**pre-process**) ，一個好的前處理可以使訓練更有效率也增加模型成效，要注意若訓練時有用前處理資料，最後實際應用時也要用相同的前處理流程處理完資料才能丟入模型中應用。

### 選擇模型、選擇超參數 

這部分算是比較困難的，模型的選擇跟參數的選擇沒有一定，由於模型訓練很費時，很難知道對於某一個問題中最佳模型及參數選擇，若要自己實做的話建議參考論文或其他人的應用結果來選擇。在 AWS SageMaker 上實際應用時有工具可以讓這部分自動化所以這邊主要專注於概念解釋。

這裡的 **超參數**  (**hyperparameter**) 主要是包含模型當中的架構選擇 (e.g. DNN 模型中要有幾層或每層要有幾個神經元) 還有訓練時要指定的參數 (e.g. 訓練時要對訓練資料跑過幾次訓練)

至於為什麼要有 **超參數** 這個名詞出現是因為要跟訓練模型時要調的 **參數** (parameter) 做一個區別，訓練時不是訓練模型的架構，模型的架構在訓練前架起來的時候就已經確定了，訓練時是訓練模型當中每一個節點的參數數值。

### 開始訓練

訓練這邊在 AWS SageMaker 除了在 SageMaker Studio 上可以自動之外，在 SageMaker notebook instance 上也可以直接 call  相關 library 自動訓練。不過這裡還是講一下一些基本名詞解釋以及簡單數學意義。

 - **batch**: 訓練時通常不會一筆資料一筆資料的看，因為太費時了，所以通常是一組一組資料的看，這一組一組的每一組就叫 batch
 - **epoch**: 訓練時，若每一個 batch 都被看過了一次，就算一個 epoch
 - **loss function**: 可以想成訓練時每個 batch 丟進去模型跟 ground truth 的誤差，要注意 loss function 是以參數作為輸入的函數
 - **optimizer**: 要進行 optimization 的演算法，ML 的 optimization 可以想成調整參數的方法
 
 至於 ML 的數學意義可以想像在一個參數空間中我們想要找到一組參數能使 **loss function** 達到最小，於是我們每看過一個 **batch** 都去計算 **loss function** ，然後對 **loss funtion** 做微分就會大致知道參數往哪個方向調整會使 **loss function** 變小，並根據 **optimizer** 去調整改參數的方向。

如果想要更深入了解 ML 後面的訓練原理跟邏輯，可以[參考](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) 這個系列講述訓練神經網絡的原理。

 ### 訓練結束測試模型
 
 訓練結束後可以用測試資料去測試模型，不過如果使用 AWS SageMaker 的話你只要給它指定訓練資料跟測試資料的話這部分也自動化了。

普遍而言，現在分類模型有 90% 正確率就算不錯，回歸模型的 **RMSE** (**方均根誤差**) 小於 1 就算可以，但是具體要求應該要以實際狀況來看。

### 應用模型

模型的應用在 AWS SageMaker 上也是有很多工具能應用，不論是在 notebook 上跑或是用 SageMaker Studio 有 model 後都能產生 Endpoint 進行互動，或是由 SageMaker 中其他功能來部屬模型至邊緣裝置上面


